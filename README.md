# MCP Toolkit Deployment - Distributed Architecture ğŸš€

**Distributed deployment across your EVA network with centralized management!**

## ğŸŒ Architecture Overview

This deployment uses a **distributed architecture** that optimizes service placement across your EVA network:

### Network Topology
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         EVA Network                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Lilith (.10)      â”‚   Adam (.11)     â”‚   MAGI Nodes          â”‚
â”‚   Primary NAS       â”‚   Business NAS   â”‚   GPU Workstations    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Shared Svcs  â”‚  â”‚   â”‚ Storage  â”‚  â”‚   â”‚ Melchior(.30)â”‚   â”‚
â”‚   â”‚ - Databases  â”‚  â”‚   â”‚ - RAIDZ1 â”‚  â”‚   â”‚ - ComfyUI    â”‚   â”‚
â”‚   â”‚ - Orchestr.  â”‚  â”‚   â”‚ - Samba  â”‚  â”‚   â”‚ - Local GPU  â”‚   â”‚
â”‚   â”‚ - Auth/APIs â”‚  â”‚   â”‚ - Backup â”‚  â”‚   â”‚ - IDE Tools  â”‚   â”‚
â”‚   â”‚ - Monitoring â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                     â”‚                  â”‚   â”‚ Caspar (.21) â”‚   â”‚
â”‚                     â”‚                  â”‚   â”‚ Balthazar(.20)â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Service Distribution Strategy

### Centralized on NAS (Lilith)
- **Databases**: Neo4j, PostgreSQL, MongoDB, Redis, Qdrant, Supabase
- **Orchestration**: MCP Orchestrator Proxy
- **Authentication**: Keycloak (future)
- **API Gateway**: Kong/Traefik (future)
- **Monitoring**: Prometheus, Grafana
- **Object Storage**: MinIO (future)
- **Embedding Models**: Small, frequently-reused models

### Local on GPU Nodes
- **ComfyUI**: Direct GPU access for image generation
- **LLMs**: Full language models remain on GPU nodes
- **IDE Tools**: Machine-specific development tools
- **Desktop MCPs**: Local filesystem access

## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/SamuraiBuddha/mcp-toolkit-deployment.git
cd mcp-toolkit-deployment
git checkout distributed-architecture
```

### 2. Deploy on Lilith (NAS)
```bash
# SSH into Lilith
ssh samuraibuddha@192.168.50.10

# Deploy shared services
docker-compose -f docker-compose.nas.yml up -d
```

### 3. Deploy on GPU Nodes
```bash
# On Melchior/Caspar/Balthazar
docker-compose -f docker-compose.local.yml up -d
```

### 4. Configure Claude Desktop

Each node needs its specific configuration:

**For Melchior (configs/melchior.json):**
```json
{
  "mcpServers": {
    "orchestrator": {
      "command": "ssh",
      "args": [
        "samuraibuddha@192.168.50.10",
        "docker", "exec", "-i", "mcp-orchestrator-proxy",
        "python", "-m", "mcp_orchestrator_proxy"
      ]
    },
    "comfyui": {
      "command": "docker",
      "args": ["exec", "-i", "comfyui-mcp", "python", "-m", "mcp_comfyui"]
    }
  }
}
```

## ğŸ® EVA Network Management Dashboard

Access the unified dashboard at: `http://192.168.50.30:3000` (or any GPU node)

### Features
- **Real-time Status**: Monitor all 5 EVA nodes
- **Container Management**: Start/stop/restart containers remotely
- **SSH Integration**: Execute commands across the network
- **Service Health**: Visual indicators for all services
- **Matrix Rain Effect**: Because style matters! ğŸ”¥

### Dashboard Deployment
```bash
cd eva-dashboard
docker-compose up -d
```

## ğŸ“ Repository Structure

```
mcp-toolkit-deployment/
â”œâ”€â”€ docker-compose.nas.yml      # Lilith deployment
â”œâ”€â”€ docker-compose.local.yml    # GPU node deployment
â”œâ”€â”€ docker-compose.dashboard.yml # EVA dashboard
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ melchior.json          # Melchior Claude config
â”‚   â”œâ”€â”€ caspar.json            # Caspar Claude config
â”‚   â”œâ”€â”€ balthazar.json         # Balthazar Claude config
â”‚   â””â”€â”€ lilith.json            # Lilith Claude config
â”œâ”€â”€ eva-dashboard/
â”‚   â”œâ”€â”€ index.html             # Dashboard UI
â”‚   â”œâ”€â”€ api-server.js          # SSH backend
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup-nas.sh           # Lilith setup
â”‚   â”œâ”€â”€ setup-gpu-node.sh      # GPU node setup
â”‚   â””â”€â”€ migrate-memory.sh      # Neo4j migration
â””â”€â”€ registry.json              # MCP discovery config
```

## ğŸ”§ Configuration

### Environment Variables
Create `.env` files on each node:

**Lilith (.env.nas):**
```env
NODE_TYPE=nas
NODE_IP=192.168.50.10
NEO4J_PASSWORD=your_secure_password
MONGO_PASSWORD=your_secure_password
GITHUB_TOKEN=your_github_token
```

**GPU Nodes (.env.local):**
```env
NODE_TYPE=gpu
NAS_IP=192.168.50.10
COMFYUI_URL=http://localhost:8188
```

### Network Requirements
- All nodes must be on `192.168.50.0/24` network
- SSH access between nodes (port 22)
- Firewall rules for service ports:
  - 8080: Orchestrator Proxy
  - 7474/7687: Neo4j
  - 5432: PostgreSQL
  - 27017: MongoDB
  - 6379: Redis
  - 3000: EVA Dashboard

## ğŸ“Š Monitoring

### Portainer
Access at `http://192.168.50.10:9443` for container management

### Logs
```bash
# View orchestrator logs
docker logs -f mcp-orchestrator-proxy

# View all logs on Lilith
ssh samuraibuddha@192.168.50.10 "docker-compose -f docker-compose.nas.yml logs -f"
```

### Resource Usage
```bash
# Check resource usage on any node
docker stats
```

## ğŸš¨ Troubleshooting

### Service Discovery Issues
```bash
# Test orchestrator from Melchior
ssh samuraibuddha@192.168.50.10 "docker exec mcp-orchestrator-proxy python -m mcp_orchestrator list"
```

### Network Connectivity
```bash
# Test SSH connection
ssh -v samuraibuddha@192.168.50.10

# Test service connectivity
curl http://192.168.50.10:8080/health
```

### Container Issues
```bash
# Restart services on Lilith
ssh samuraibuddha@192.168.50.10 "docker-compose -f docker-compose.nas.yml restart"
```

## ğŸ”„ Migration Guide

### From Single-Host to Distributed

1. **Backup Current Data**
   ```bash
   ./scripts/backup-neo4j.sh
   ```

2. **Deploy NAS Services**
   ```bash
   ssh samuraibuddha@192.168.50.10
   cd mcp-toolkit-deployment
   docker-compose -f docker-compose.nas.yml up -d
   ```

3. **Migrate Memory Database**
   ```bash
   ./scripts/migrate-memory.sh
   ```

4. **Update Claude Configs**
   - Replace local configs with network-aware versions
   - Point to Lilith's orchestrator

5. **Test Everything**
   ```bash
   # From any Claude Desktop
   # Try: "What do you remember about our projects?"
   ```

## ğŸ¯ Performance Optimization

### Why This Architecture?
- **Latency**: GPU workloads stay local for minimal latency
- **Sharing**: Databases centralized for cross-node access
- **Backup**: All persistent data on ZFS with snapshots
- **Scaling**: Add more GPU nodes without database duplication

### Resource Allocation
- **Lilith**: High CPU for database operations
- **GPU Nodes**: Maximum GPU/RAM for AI workloads
- **Network**: 1Gbps minimum, 10Gbps recommended

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/awesome-addition`
3. Test on your EVA network
4. Submit pull request

## ğŸ“ License

MIT License - Built for the EVA network community!

---

*Part of the EVA Network Infrastructure - Where AI meets Evangelion! ğŸ¤–*

*For standard Claude Desktop deployments, see [mcp-orchestrator](https://github.com/SamuraiBuddha/mcp-orchestrator)*